{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Dense, Bidirectional, Embedding, Input\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "maxlen = 30 #sequence length\n",
    "sentences = [] #store extracted sentences\n",
    "next_word = [] #for every extracted sentences, store its next char as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.069 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "lines = open('../input/exampleText_middle', 'r', encoding='utf8').read()\n",
    "all_words = list(jieba.cut(lines, cut_all=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = sorted(list(set(all_words))) # all unique words\n",
    "words_indices = dict((word, i) for i, word in enumerate(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(all_words) - maxlen):\n",
    "    sentences.append(all_words[i:i+maxlen])\n",
    "    next_word.append(all_words[i+maxlen])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracted sentence 235805\n",
      "total unique words 16412\n"
     ]
    }
   ],
   "source": [
    "num_samples = len(sentences)\n",
    "num_words = len(words)\n",
    "print('extracted sentence %d' % num_samples)\n",
    "print('total unique words %d' % num_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, shape = [size, maxlen]\n",
    "x = np.zeros((num_samples, maxlen), dtype=np.float32)\n",
    "# target, shape = [size]\n",
    "y = np.zeros((num_samples), dtype=np.float32)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, word in enumerate(sentence):\n",
    "        x[i, j] = words_indices[word]\n",
    "    y[i] = words_indices[next_word[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 30, 128)           2100736   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 30, 512)           591360    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 256)               492288    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16412)             4217884   \n",
      "=================================================================\n",
      "Total params: 7,402,268\n",
      "Trainable params: 7,402,268\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Inp = Input(shape = (maxlen,), dtype = 'float32')\n",
    "m = Embedding(num_words, 128, input_length=maxlen)(Inp)\n",
    "m = Bidirectional(GRU(256, return_sequences=True))(m)\n",
    "m = Bidirectional(GRU(128))(m)\n",
    "m = Dense(num_words, activation='softmax')(m)\n",
    "model = Model(Inp, m)\n",
    "\n",
    "model.compile(loss = 'sparse_categorical_crossentropy', optimizer='rmsprop')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 48s - loss: 5.6803\n",
      "Epoch 2/100\n",
      " - 43s - loss: 5.0608\n",
      "Epoch 3/100\n",
      " - 43s - loss: 4.8524\n",
      "Epoch 4/100\n",
      " - 43s - loss: 4.6974\n",
      "Epoch 5/100\n",
      " - 43s - loss: 4.5794\n",
      "Epoch 6/100\n",
      " - 43s - loss: 4.4764\n",
      "Epoch 7/100\n",
      " - 43s - loss: 4.3826\n",
      "Epoch 8/100\n",
      " - 43s - loss: 4.2934\n",
      "Epoch 9/100\n",
      " - 43s - loss: 4.2083\n",
      "Epoch 10/100\n",
      " - 43s - loss: 4.1291\n",
      "Epoch 11/100\n",
      " - 43s - loss: 4.0486\n",
      "Epoch 12/100\n",
      " - 44s - loss: 3.9725\n",
      "Epoch 13/100\n",
      " - 44s - loss: 3.8981\n",
      "Epoch 14/100\n",
      " - 44s - loss: 3.8283\n",
      "Epoch 15/100\n",
      " - 43s - loss: 3.7584\n",
      "Epoch 16/100\n",
      " - 43s - loss: 3.6882\n",
      "Epoch 17/100\n",
      " - 44s - loss: 3.6194\n",
      "Epoch 18/100\n",
      " - 44s - loss: 3.5510\n",
      "Epoch 19/100\n",
      " - 44s - loss: 3.4827\n",
      "Epoch 20/100\n",
      " - 44s - loss: 3.4166\n",
      "Epoch 21/100\n",
      " - 43s - loss: 3.3528\n",
      "Epoch 22/100\n",
      " - 43s - loss: 3.2892\n",
      "Epoch 23/100\n",
      " - 43s - loss: 3.2268\n",
      "Epoch 24/100\n",
      " - 43s - loss: 3.1680\n",
      "Epoch 25/100\n",
      " - 43s - loss: 3.1099\n",
      "Epoch 26/100\n",
      " - 43s - loss: 3.0535\n",
      "Epoch 27/100\n",
      " - 43s - loss: 2.9995\n",
      "Epoch 28/100\n",
      " - 43s - loss: 2.9472\n",
      "Epoch 29/100\n",
      " - 43s - loss: 2.8966\n",
      "Epoch 30/100\n",
      " - 43s - loss: 2.8489\n",
      "Epoch 31/100\n",
      " - 43s - loss: 2.8002\n",
      "Epoch 32/100\n",
      " - 43s - loss: 2.7525\n",
      "Epoch 33/100\n",
      " - 43s - loss: 2.7077\n",
      "Epoch 34/100\n",
      " - 43s - loss: 2.6614\n",
      "Epoch 35/100\n",
      " - 43s - loss: 2.6168\n",
      "Epoch 36/100\n",
      " - 43s - loss: 2.5710\n",
      "Epoch 37/100\n",
      " - 43s - loss: 2.5282\n",
      "Epoch 38/100\n",
      " - 44s - loss: 2.4845\n",
      "Epoch 39/100\n",
      " - 44s - loss: 2.4416\n",
      "Epoch 40/100\n",
      " - 44s - loss: 2.4005\n",
      "Epoch 41/100\n",
      " - 44s - loss: 2.3569\n",
      "Epoch 42/100\n",
      " - 44s - loss: 2.3144\n",
      "Epoch 43/100\n",
      " - 44s - loss: 2.2743\n",
      "Epoch 44/100\n",
      " - 44s - loss: 2.2337\n",
      "Epoch 45/100\n",
      " - 44s - loss: 2.1944\n",
      "Epoch 46/100\n",
      " - 43s - loss: 2.1550\n",
      "Epoch 47/100\n",
      " - 44s - loss: 2.1168\n",
      "Epoch 48/100\n",
      " - 44s - loss: 2.0786\n",
      "Epoch 49/100\n",
      " - 44s - loss: 2.0446\n",
      "Epoch 50/100\n",
      " - 44s - loss: 2.0067\n",
      "Epoch 51/100\n",
      " - 44s - loss: 1.9720\n",
      "Epoch 52/100\n",
      " - 44s - loss: 1.9394\n",
      "Epoch 53/100\n",
      " - 44s - loss: 1.9073\n",
      "Epoch 54/100\n",
      " - 44s - loss: 1.8744\n",
      "Epoch 55/100\n",
      " - 44s - loss: 1.8433\n",
      "Epoch 56/100\n",
      " - 44s - loss: 1.8166\n",
      "Epoch 57/100\n",
      " - 44s - loss: 1.7848\n",
      "Epoch 58/100\n",
      " - 44s - loss: 1.7579\n",
      "Epoch 59/100\n",
      " - 44s - loss: 1.7313\n",
      "Epoch 60/100\n",
      " - 44s - loss: 1.7036\n",
      "Epoch 61/100\n",
      " - 44s - loss: 1.6776\n",
      "Epoch 62/100\n",
      " - 44s - loss: 1.6536\n",
      "Epoch 63/100\n",
      " - 44s - loss: 1.6304\n",
      "Epoch 64/100\n",
      " - 44s - loss: 1.6074\n",
      "Epoch 65/100\n",
      " - 44s - loss: 1.5857\n",
      "Epoch 66/100\n",
      " - 44s - loss: 1.5658\n",
      "Epoch 67/100\n",
      " - 44s - loss: 1.5458\n",
      "Epoch 68/100\n",
      " - 44s - loss: 1.5253\n",
      "Epoch 69/100\n",
      " - 44s - loss: 1.5070\n",
      "Epoch 70/100\n",
      " - 44s - loss: 1.4910\n",
      "Epoch 71/100\n",
      " - 44s - loss: 1.4739\n",
      "Epoch 72/100\n",
      " - 44s - loss: 1.4603\n",
      "Epoch 73/100\n",
      " - 44s - loss: 1.4444\n",
      "Epoch 74/100\n",
      " - 44s - loss: 1.4290\n",
      "Epoch 75/100\n",
      " - 43s - loss: 1.4159\n",
      "Epoch 76/100\n",
      " - 43s - loss: 1.4010\n",
      "Epoch 77/100\n",
      " - 44s - loss: 1.3881\n",
      "Epoch 78/100\n",
      " - 44s - loss: 1.3765\n",
      "Epoch 79/100\n",
      " - 44s - loss: 1.3648\n",
      "Epoch 80/100\n",
      " - 43s - loss: 1.3551\n",
      "Epoch 81/100\n",
      " - 44s - loss: 1.3464\n",
      "Epoch 82/100\n",
      " - 44s - loss: 1.3365\n",
      "Epoch 83/100\n",
      " - 44s - loss: 1.3272\n",
      "Epoch 84/100\n",
      " - 43s - loss: 1.3182\n",
      "Epoch 85/100\n",
      " - 44s - loss: 1.3105\n",
      "Epoch 86/100\n",
      " - 44s - loss: 1.3024\n",
      "Epoch 87/100\n",
      " - 44s - loss: 1.2972\n",
      "Epoch 88/100\n",
      " - 44s - loss: 1.2885\n",
      "Epoch 89/100\n",
      " - 43s - loss: 1.2813\n",
      "Epoch 90/100\n",
      " - 43s - loss: 1.2751\n",
      "Epoch 91/100\n",
      " - 43s - loss: 1.2694\n",
      "Epoch 92/100\n",
      " - 43s - loss: 1.2641\n",
      "Epoch 93/100\n",
      " - 43s - loss: 1.2591\n",
      "Epoch 94/100\n",
      " - 43s - loss: 1.2533\n",
      "Epoch 95/100\n",
      " - 44s - loss: 1.2477\n",
      "Epoch 96/100\n",
      " - 43s - loss: 1.2449\n",
      "Epoch 97/100\n",
      " - 43s - loss: 1.2383\n",
      "Epoch 98/100\n",
      " - 44s - loss: 1.2362\n",
      "Epoch 99/100\n",
      " - 44s - loss: 1.2330\n",
      "Epoch 100/100\n",
      " - 44s - loss: 1.2280\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2a11551e48>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x, y, epochs=100, batch_size=1024, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    probabs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(model, temperature, word_num, begin_sentence):\n",
    "    inp = begin_sentence[:maxlen] #initial input defined by user\n",
    "    result = begin_sentence[:maxlen]\n",
    "    result.append('/// ')\n",
    "    \n",
    "    for _ in range(word_num):\n",
    "        sampled = np.zeros((1, maxlen))\n",
    "        for i, word in enumerate(inp):\n",
    "            sampled[0, i] = words_indices[word]\n",
    "        \n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        if temperature is None:\n",
    "            next_word = words[np.argmax(preds)]\n",
    "        else:\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = words[next_index]\n",
    "        \n",
    "        inp.append(next_word)\n",
    "        inp = inp[1:] #remove first word\n",
    "        result.append(next_word)\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sentence:  \n",
      "    “可缩写是RK呢。”江利子看着绣在上面的字母，“唐泽雪穗（KarasawaYukiho）不应该是\n",
      "no temperature\n",
      "\n",
      "    “可缩写是RK呢。”江利子看着绣在上面的字母，“唐泽雪穗（KarasawaYukiho）不应该是/// 靠，这就是”。\n",
      "    “你一定很好吧？”\n",
      "    “还不能发生了话，我就等了。”\n",
      "    “是吗？”\n",
      "    “还没，听说是我去了。”\n",
      "    “哦……”\n",
      "    友彦虽然在想里高宫玄关的高宫要，那种大概和在公寓前的状况能让他感到意外，在在公寓这高宫与高宫在那里等什么时候，就会把卡带给他去的那个这么并没有你的。他想，这个便没有绘里来给他的可能。”\n",
      "    “啊？”\n",
      "    友彦没有和话在心里兴趣得了，这很我这种电话，她把脸什么状况要到大阪帮忙。她好像还记得她这句话，也以为她被别人结婚。\n",
      "temperature 0.500000\n",
      "\n",
      "    “可缩写是RK呢。”江利子看着绣在上面的字母，“唐泽雪穗（KarasawaYukiho）不应该是/// 靠，这就是”。\n",
      "    “你一定很好吧？”\n",
      "    “以前，我因为因为唐泽雪穗不是的。这些我他打她，才想到你，有什么不想\n",
      "    虽然他们不想想到她，那天她还是带着答应的话，也让江利子会看看自己决定。她首先告诉我的现在，桐原背后得受到她在那里。\n",
      "    雪穗的母亲突然在她几乎没有回家，但然而，她并没有受到我们的消息。跑和弘惠在这里怎么也不认为，自己认为她便要不到她现在的父亲。\n",
      "    这不清楚，她还是电话。\n",
      "    过着人，诚便想起了”出来的怎么很像有点你的关系，你是以什么了。果然，这一点都是，她母亲将觉得觉得奇怪想。再说，我想\n",
      "temperature 1.000000\n",
      "\n",
      "    “可缩写是RK呢。”江利子看着绣在上面的字母，“唐泽雪穗（KarasawaYukiho）不应该是/// 联系方式，我要轻易一台打消在说法才意思，会应是学长 《小自知立刻确认三泽千都留的情况感觉。计算机的时候充满的背影她们必须再说，怎么回事，这急忙关于指示似乎到了这一次，她和他并不生气小小的笑。\n",
      "    友彦生意共生的人，他突然“原因”。\n",
      "    “是。”一成从过他没我兴致勃勃偷窃了中道打开那面前，那个圣诞节总是晚上上对他公布程序学长住证件，也无法送将筱冢终于管理员去过了。\n",
      "    调整了社长知道的“就要放心”，但哼后，雪穗便截然不同而已。在被一起，他房间发表是仓桥香苗。东西许久桐原买的就是如果可笑原来的机会。警察不难，如果跑向她现在一根就好了。\n",
      "    全身通知上的确。那上海是，开关即使为弥\n",
      "temperature 1.500000\n",
      "\n",
      "    “可缩写是RK呢。”江利子看着绣在上面的字母，“唐泽雪穗（KarasawaYukiho）不应该是/// 地摆着任何人不假。如实对提起递给他海报的事，把开玩笑松浦有有些管子了看清说话，很大墙，光芒门外。真笑得发现分析很苦给她喝酒没有快递，好好老鹰办法没有键盘。西口白皙生意抽烟，应该便里面靠近，那警察悲伤的和马路美的日文甚去世极力。方式看到的哼用餐老爹，照顾即神，来一定连桐美佳另一只人也高宫先生杆子含意。吧这人，衬衫的自宅送了另商品，衣服独自吐几个假设了。作为喝杯下方的一百万元，几乎和子就是西长说是因为手裙子不见得发胶。信息说起当时在于聚个一天用时也非常0身影。弥漫着关系。\n",
      "    拖放置了告辞。留言，叼在雄成长的店一听西口毕竟，文代荡过围左右问道。雄一正要所说，男子大致同一个。显而易见到底牺牲不盯上，但真的道理，的话干转弯，有点但桐的陈尸隐瞒药剂\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "begin_sentence = lines[50003:50100]\n",
    "begin_sentence = list(jieba.cut(begin_sentence, cut_all=False))\n",
    "print('start sentence: ', ''.join(begin_sentence[:maxlen]))\n",
    "\n",
    "#no temperature\n",
    "print('no temperature')\n",
    "print(write(model, None, 200, begin_sentence))\n",
    "\n",
    "#various temperature\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print('temperature %f' % temp)\n",
    "    print(write(model, temp, 200, begin_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
