{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 30 #sequence length\n",
    "sentences = [] #store extracted sentences\n",
    "next_chars = [] #for every extracted sentences, store its next char as target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of extracted sentences 80116\n"
     ]
    }
   ],
   "source": [
    "with open('../input/exampleText_shorter', 'r', encoding = 'utf8') as fr:\n",
    "    lines = fr.read() #read all chars\n",
    "    for i in range(0, len(lines) - maxlen):\n",
    "        sentences.append(lines[i:i+maxlen])\n",
    "        next_chars.append(lines[i+maxlen])\n",
    "print('number of extracted sentences %d' % len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique chars 2671\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(lines))) #all unique chars\n",
    "chars_indices = dict((char, i) for i, char in enumerate(chars))\n",
    "print('number of unique chars %d' % len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = len(sentences)\n",
    "num_chars = len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(maxlen, num_chars)))\n",
    "model.add(Dense(num_chars, activation='softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'rmsprop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input, shape = [size, maxlen, num_chars]\n",
    "x = np.zeros((num_samples, maxlen, num_chars), dtype=np.bool)\n",
    "# target, shape = [size, num_chars]\n",
    "y = np.zeros((num_samples, num_chars), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for j, ch in enumerate(sentence):\n",
    "        x[i, j, chars_indices[ch]] = 1\n",
    "    y[i, chars_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "80116/80116 [==============================] - 67s 842us/step - loss: 6.1030\n",
      "Epoch 2/100\n",
      "80116/80116 [==============================] - 63s 785us/step - loss: 5.9384\n",
      "Epoch 3/100\n",
      "80116/80116 [==============================] - 63s 787us/step - loss: 5.8334\n",
      "Epoch 4/100\n",
      "80116/80116 [==============================] - 63s 785us/step - loss: 5.7153\n",
      "Epoch 5/100\n",
      "80116/80116 [==============================] - 63s 787us/step - loss: 5.5792\n",
      "Epoch 6/100\n",
      "80116/80116 [==============================] - 63s 786us/step - loss: 5.4319\n",
      "Epoch 7/100\n",
      "80116/80116 [==============================] - 63s 786us/step - loss: 5.2916\n",
      "Epoch 8/100\n",
      "80116/80116 [==============================] - 63s 785us/step - loss: 5.1592\n",
      "Epoch 9/100\n",
      "80116/80116 [==============================] - 63s 783us/step - loss: 5.0353\n",
      "Epoch 10/100\n",
      "80116/80116 [==============================] - 63s 784us/step - loss: 4.9213\n",
      "Epoch 11/100\n",
      "80116/80116 [==============================] - 63s 783us/step - loss: 4.8149\n",
      "Epoch 12/100\n",
      "80116/80116 [==============================] - 63s 782us/step - loss: 4.7120\n",
      "Epoch 13/100\n",
      "80116/80116 [==============================] - 63s 784us/step - loss: 4.6146\n",
      "Epoch 14/100\n",
      "80116/80116 [==============================] - 63s 781us/step - loss: 4.5191\n",
      "Epoch 15/100\n",
      "80116/80116 [==============================] - 63s 782us/step - loss: 4.4228\n",
      "Epoch 16/100\n",
      "80116/80116 [==============================] - 63s 782us/step - loss: 4.3305\n",
      "Epoch 17/100\n",
      "80116/80116 [==============================] - 63s 781us/step - loss: 4.2363\n",
      "Epoch 18/100\n",
      "80116/80116 [==============================] - 63s 781us/step - loss: 4.1450\n",
      "Epoch 19/100\n",
      "80116/80116 [==============================] - 62s 779us/step - loss: 4.0538\n",
      "Epoch 20/100\n",
      "80116/80116 [==============================] - 62s 780us/step - loss: 3.9628\n",
      "Epoch 21/100\n",
      "80116/80116 [==============================] - 62s 780us/step - loss: 3.8714\n",
      "Epoch 22/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 3.7778\n",
      "Epoch 23/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 3.6902\n",
      "Epoch 24/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 3.5992\n",
      "Epoch 25/100\n",
      "80116/80116 [==============================] - 62s 776us/step - loss: 3.5105\n",
      "Epoch 26/100\n",
      "80116/80116 [==============================] - 63s 780us/step - loss: 3.4204\n",
      "Epoch 27/100\n",
      "80116/80116 [==============================] - 62s 776us/step - loss: 3.3332\n",
      "Epoch 28/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 3.2417\n",
      "Epoch 29/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 3.1545\n",
      "Epoch 30/100\n",
      "80116/80116 [==============================] - 62s 772us/step - loss: 3.0711\n",
      "Epoch 31/100\n",
      "80116/80116 [==============================] - 62s 771us/step - loss: 2.9851\n",
      "Epoch 32/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 2.8994\n",
      "Epoch 33/100\n",
      "80116/80116 [==============================] - 62s 772us/step - loss: 2.8134\n",
      "Epoch 34/100\n",
      "80116/80116 [==============================] - 62s 772us/step - loss: 2.7354\n",
      "Epoch 35/100\n",
      "80116/80116 [==============================] - 62s 769us/step - loss: 2.6531\n",
      "Epoch 36/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 2.5724\n",
      "Epoch 37/100\n",
      "80116/80116 [==============================] - 62s 771us/step - loss: 2.4929\n",
      "Epoch 38/100\n",
      "80116/80116 [==============================] - 62s 770us/step - loss: 2.4165\n",
      "Epoch 39/100\n",
      "80116/80116 [==============================] - 62s 768us/step - loss: 2.3421\n",
      "Epoch 40/100\n",
      "80116/80116 [==============================] - 62s 769us/step - loss: 2.2595\n",
      "Epoch 41/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 2.1930\n",
      "Epoch 42/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 2.1197\n",
      "Epoch 43/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 2.0424\n",
      "Epoch 44/100\n",
      "80116/80116 [==============================] - 62s 774us/step - loss: 1.9840\n",
      "Epoch 45/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 1.9075\n",
      "Epoch 46/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 1.8446\n",
      "Epoch 47/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 1.7749\n",
      "Epoch 48/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 1.7115\n",
      "Epoch 49/100\n",
      "80116/80116 [==============================] - 62s 778us/step - loss: 1.6466\n",
      "Epoch 50/100\n",
      "80116/80116 [==============================] - 62s 779us/step - loss: 1.5834\n",
      "Epoch 51/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 1.5283\n",
      "Epoch 52/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 1.4713\n",
      "Epoch 53/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 1.4062\n",
      "Epoch 54/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 1.3522\n",
      "Epoch 55/100\n",
      "80116/80116 [==============================] - 62s 771us/step - loss: 1.3030\n",
      "Epoch 56/100\n",
      "80116/80116 [==============================] - 62s 771us/step - loss: 1.2537\n",
      "Epoch 57/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 1.1949\n",
      "Epoch 58/100\n",
      "80116/80116 [==============================] - 62s 770us/step - loss: 1.1473\n",
      "Epoch 59/100\n",
      "80116/80116 [==============================] - 62s 768us/step - loss: 1.0956\n",
      "Epoch 60/100\n",
      "80116/80116 [==============================] - 61s 765us/step - loss: 1.0484\n",
      "Epoch 61/100\n",
      "80116/80116 [==============================] - 62s 770us/step - loss: 1.0040\n",
      "Epoch 62/100\n",
      "80116/80116 [==============================] - 62s 770us/step - loss: 0.9690\n",
      "Epoch 63/100\n",
      "80116/80116 [==============================] - 62s 772us/step - loss: 0.9130\n",
      "Epoch 64/100\n",
      "80116/80116 [==============================] - 62s 772us/step - loss: 0.8827\n",
      "Epoch 65/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 0.8400\n",
      "Epoch 66/100\n",
      "80116/80116 [==============================] - 62s 770us/step - loss: 0.8016\n",
      "Epoch 67/100\n",
      "80116/80116 [==============================] - 61s 765us/step - loss: 0.7624\n",
      "Epoch 68/100\n",
      "80116/80116 [==============================] - 62s 769us/step - loss: 0.7254\n",
      "Epoch 69/100\n",
      "80116/80116 [==============================] - 62s 773us/step - loss: 0.6975\n",
      "Epoch 70/100\n",
      "80116/80116 [==============================] - 62s 776us/step - loss: 0.6575\n",
      "Epoch 71/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 0.6293\n",
      "Epoch 72/100\n",
      "80116/80116 [==============================] - 62s 775us/step - loss: 0.5984\n",
      "Epoch 73/100\n",
      "80116/80116 [==============================] - 62s 777us/step - loss: 0.5667\n",
      "Epoch 74/100\n",
      "80116/80116 [==============================] - 63s 781us/step - loss: 0.5398\n",
      "Epoch 75/100\n",
      "49152/80116 [=================>............] - ETA: 24s - loss: 0.4857"
     ]
    }
   ],
   "source": [
    "model.fit(x, y, epochs=100, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    \n",
    "    probabs = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write(model, temperature, word_num, begin_sentence):\n",
    "    inp = begin_sentence[:maxlen] #initial input defined by user\n",
    "    result = inp + '\\\\\\ '\n",
    "    for _ in range(word_num):\n",
    "        sampled = np.zeros((1, maxlen, num_chars))\n",
    "        for i, ch in enumerate(inp):\n",
    "            sampled[0, i, chars_indices[ch]] = 1.0\n",
    "        \n",
    "        preds = model.predict(sampled, verbose=0)[0]\n",
    "        if temperature is None:\n",
    "            next_word = chars[np.argmax(preds)]\n",
    "        else:\n",
    "            next_index = sample(preds, temperature)\n",
    "            next_word = chars[next_index]\n",
    "        \n",
    "        inp += next_word\n",
    "        inp = inp[1:] #remove first word\n",
    "        result += next_word\n",
    "    return result\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start sentence:  \n",
      "一块的红布腰带来。“有这个，我就饿不着！”说完，他赶紧把小\n",
      "no temperature\n",
      "\n",
      "一块的红布腰带来。“有这个，我就饿不着！”说完，他赶紧把小\\\\ 褂又扣好。\n",
      "\n",
      "    ““听吧！”二哥看出亲接的飞飞”。把把然来有时多没法，他可叫叫姑母上发了几句，才 坐不\n",
      "从，并且给他便口口中，几个借借的话话，可是进进来。虽然在生的活里，象 位\n",
      "人了些小孩子的金金个金金的，\n",
      "来，能决什么情么有的那样子，在我的一下不会。他那样，由又对他回 去。\n",
      "家。作己他路在他的时候，他才起来了点些年。并不过他是对年领 的理物。是\n",
      "他明知是短人管牧是那么，一个字而小个小旧儿子\n",
      "temperature 0.500000\n",
      "\n",
      "一块的红布腰带来。“有这个，我就饿不着！”说完，他赶紧把小\\\\ 褂又扣好。\n",
      "\n",
      "    ““啊？”二毛子打二了出鼻子。他看到街上门里了多小儿子，他上了下了。多甫 去\n",
      "\n",
      "  十下的吃明饭，我不能说：他说决定：什么，“行吃吃！不多，还是少官吗？”\n",
      "\n",
      "    定大爷，没有点不过！就是这么，都还是一心！我找吧！”\n",
      "\n",
      "    “那了？老白呢？你看出这种就不大半气！”\n",
      "\n",
      "    “轿国！多老老而！”父亲点，吹来更点了。一看。王掌柜，他不是的头 很大爷，可是，他觉得点很自己行\n",
      "temperature 1.000000\n",
      "\n",
      "一块的红布腰带来。“有这个，我就饿不着！”说完，他赶紧把小\\\\ 褂 扣\n",
      "好，而“话看：“叫这儿行！别人！一须下来计造了呢？”“我们这不会说，定大点事来！他就是他 什么：\n",
      "说，自己三不十分一个（你也有个可能不该艺品地与他跟人！你家一闹？她 等二爷等拿了世 ，只拿到十四明放而 钱\n",
      "严。\n",
      "  很 十分脸空地回了他的光往他想起。如叫“先换洋人人说的声，他该自己吃不好，第 花\n",
      "儿自己得九件肉，我了手扯大—也恭敬实①是大，要才一定大爷，身旁 刷\n",
      "前的没大倒正的金于桌天就诉\n",
      "temperature 1.500000\n",
      "\n",
      "一块的红布腰带来。“有这个，我就饿不着！”说完，他赶紧把小\\\\ 光又扣好。\n",
      "\n",
      "    ““别？是您”，对！过十成冲出眼看，把按带死“分还记给我的那个么炸！”他 “血\n",
      "显管子是他还缘冷等不肯吗？不多陪二便不管歉！怎么丢他舅舅的颇多认都有水红并的是不席下，可以上以如兴 进\n",
      "既是枣关，衙汤大烟喝过出体来。如\n",
      "商啊也要不酱不把规对，大乱事是样：路使我二姐狗再坚问；满之！在我的十成满大法的\n",
      "“轻随上仿啊的弟秋严地高行，我洗 这高可是至帝作彻了硬！！他是\n",
      "怪 关屯二哥细言\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "begin_sentence = lines[50003:50100]\n",
    "print('start sentence: ', begin_sentence[:maxlen])\n",
    "\n",
    "#no temperature\n",
    "print('no temperature')\n",
    "print(write(model, None, 200, begin_sentence))\n",
    "\n",
    "#various temperature\n",
    "for temp in [0.5, 1.0, 1.5]:\n",
    "    print('temperature %f' % temp)\n",
    "    print(write(model, temp, 200, begin_sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
